{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-PuJl-rHxW1"
   },
   "source": [
    "Neural networks can perform really powerful tasks. We're going to use them to do something relatively simple, but this framework can be expanded to solve more complex problems.\n",
    "\n",
    "In this exercise, we're going to use a dataset of handwritten letters called EMNIST. This dataset is a subset of a larger dataset composed of handwritten letters and numbers that was orginally collected from census bureau employees and high school students. \n",
    "\n",
    "Because different people write letters differently, programming a neural network to recognize handwritten letters and convert them to typed text is a non-trivial task. For example, think about all of the different ways you've seen people write the letter \"J.\"\n",
    "\n",
    "**STEP 1**\n",
    "\n",
    "The first thing we're going to do is import the EMNIST libraries so that we have access to this training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bGP5woydMEMa",
    "outputId": "99962e5d-57f7-4376-88fd-b4da68654cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-mnist'...\n",
      "remote: Enumerating objects: 217, done.\u001b[K\n",
      "remote: Total 217 (delta 0), reused 0 (delta 0), pack-reused 217\u001b[K\n",
      "Receiving objects: 100% (217/217), 41.85 KiB | 244.00 KiB/s, done.\n",
      "Resolving deltas: 100% (105/105), done.\n",
      "--2019-09-19 14:35:29--  http://yann.lecun.com/exdb/mnist/\n",
      "Resolving yann.lecun.com (yann.lecun.com)... 216.165.22.6\n",
      "Connecting to yann.lecun.com (yann.lecun.com)|216.165.22.6|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29170 (28K) [text/html]\n",
      "Saving to: ‘data/index.html.tmp’\n",
      "\n",
      "index.html.tmp      100%[===================>]  28.49K   129KB/s    in 0.2s    \n",
      "\n",
      "2019-09-19 14:35:30 (129 KB/s) - ‘data/index.html.tmp’ saved [29170/29170]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2019-09-19 14:35:30--  http://yann.lecun.com/robots.txt\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2019-09-19 14:35:30 ERROR 404: Not Found.\n",
      "\n",
      "Removing data/index.html.tmp since it should be rejected.\n",
      "\n",
      "--2019-09-19 14:35:30--  http://yann.lecun.com/\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38324 (37K) [text/html]\n",
      "Saving to: ‘data/index.html.tmp’\n",
      "\n",
      "index.html.tmp      100%[===================>]  37.43K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2019-09-19 14:35:31 (10.3 MB/s) - ‘data/index.html.tmp’ saved [38324/38324]\n",
      "\n",
      "Removing data/index.html.tmp since it should be rejected.\n",
      "\n",
      "--2019-09-19 14:35:31--  http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9912422 (9.5M) [application/x-gzip]\n",
      "Saving to: ‘data/train-images-idx3-ubyte.gz’\n",
      "\n",
      "train-images-idx3-u 100%[===================>]   9.45M  1.65MB/s    in 9.9s    \n",
      "\n",
      "2019-09-19 14:35:41 (979 KB/s) - ‘data/train-images-idx3-ubyte.gz’ saved [9912422/9912422]\n",
      "\n",
      "--2019-09-19 14:35:41--  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28881 (28K) [application/x-gzip]\n",
      "Saving to: ‘data/train-labels-idx1-ubyte.gz’\n",
      "\n",
      "train-labels-idx1-u 100%[===================>]  28.20K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2019-09-19 14:35:41 (6.77 MB/s) - ‘data/train-labels-idx1-ubyte.gz’ saved [28881/28881]\n",
      "\n",
      "--2019-09-19 14:35:41--  http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1648877 (1.6M) [application/x-gzip]\n",
      "Saving to: ‘data/t10k-images-idx3-ubyte.gz’\n",
      "\n",
      "t10k-images-idx3-ub 100%[===================>]   1.57M  2.18MB/s    in 0.7s    \n",
      "\n",
      "2019-09-19 14:35:42 (2.18 MB/s) - ‘data/t10k-images-idx3-ubyte.gz’ saved [1648877/1648877]\n",
      "\n",
      "--2019-09-19 14:35:42--  http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Reusing existing connection to yann.lecun.com:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4542 (4.4K) [application/x-gzip]\n",
      "Saving to: ‘data/t10k-labels-idx1-ubyte.gz’\n",
      "\n",
      "t10k-labels-idx1-ub 100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-09-19 14:35:42 (111 MB/s) - ‘data/t10k-labels-idx1-ubyte.gz’ saved [4542/4542]\n",
      "\n",
      "FINISHED --2019-09-19 14:35:42--\n",
      "Total wall clock time: 13s\n",
      "Downloaded: 6 files, 11M in 11s (1.03 MB/s)\n",
      "~/Documents/Practice/Hand_Writing_Recognizer/data ~/Documents/Practice/Hand_Writing_Recognizer\n",
      "~/Documents/Practice/Hand_Writing_Recognizer\n",
      "Collecting emnist\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/f4/78b24acbef9e8fe976dda700f16a3606f3b8363b015bc555f8050fbbd8ac/emnist-0.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /Users/tebbythomas/miniconda3/envs/py3/lib/python3.6/site-packages (from emnist) (1.16.4)\n",
      "Requirement already satisfied: requests in /Users/tebbythomas/miniconda3/envs/py3/lib/python3.6/site-packages (from emnist) (2.14.2)\n",
      "Requirement already satisfied: tqdm in /Users/tebbythomas/miniconda3/envs/py3/lib/python3.6/site-packages (from emnist) (4.32.2)\n",
      "Installing collected packages: emnist\n",
      "Successfully installed emnist-0.0\n",
      "Imported the EMNIST libraries we need!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1.1\n",
    "\n",
    "!git clone https://github.com/sorki/python-mnist\n",
    "!./python-mnist/get_data.sh\n",
    "!pip3 install emnist\n",
    "from emnist import extract_training_samples\n",
    "\n",
    "print(\"Imported the EMNIST libraries we need!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsAUNl0jQ24H"
   },
   "source": [
    "Now, we want to grab the data that we want to analyze from these libraries.  In this case, we want to grab the EMNIST \"letters\" dataset, which is made up of 145,600 28x28 pixel (or 784 pixels in total) images of letters. Each pixel is a grayscale value between 0 and 255. To make it easier for the neural network to process, we divide each value by 255 to get to a number between 0 and 1 for each pixel in each image. Performing a transformation like this to make the data easier to process in a machine learning method is called pre-processing.  Different pre-processing steps may be required for different types of data. \n",
    "\n",
    "This dataset also includes the known labels for these images. That's what our neural network is going to learn from. In this case, we'll use the first 60,000 pictures as our training set and then we'll use the following 10,000 pictures as our testing set (and use them to ask the network to check how well it has learned). This is a bit of an arbitrary decision, but based on our experience, this is large enough of a sample (more will just take longer to process but won't improve accuracy of the model) --- but don't take our word for it, you can play around with the training and testing sample on your own. Just remember to always keep the two samples separate.\n",
    "\n",
    "When you run this cell, it might take a little bit of time -- remember that this code is essentially downloading 70,000 pictures!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vDj9tyEAMVn5",
    "outputId": "e5f78549-2669-44b1-b55d-ef082b5b1304"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7941e0f7763b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Grab the data from the OpenML website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# X will be our images and y will be the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_training_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'letters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make sure that every pixel in all of the images is a value between 0 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mextract_training_samples\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \"\"\"Extract the training samples for a given dataset as a pair of numpy arrays, (images, labels). The dataset must be\n\u001b[1;32m    208\u001b[0m     one of those listed by list_datasets(), e.g. 'digits' or 'mnist'.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mextract_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mextract_samples\u001b[0;34m(dataset, usage)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \"\"\"Extract the samples for a given dataset and usage as a pair of numpy arrays, (images, labels). The dataset must\n\u001b[1;32m    198\u001b[0m     be one of those listed by list_datasets(), e.g. 'digits' or 'mnist'. Usage should be either 'train' or 'test'.\"\"\"\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(dataset, usage, component)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unrecognized value %r for component. Expected 'images' or 'labels'.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mensure_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mcache_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cached_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mzip_internal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZIP_PATH_TEMPLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mensure_cached_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfirst_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No source URLs listed in SOURCE_URLS!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfirst_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mensure_cached_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'drive.google.com'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mdownload_large_google_drive_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/emnist/__init__.py\u001b[0m in \u001b[0;36mdownload_large_google_drive_file\u001b[0;34m(url, save_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m     hosting is advisable in the future.\"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "# STEP 1.2\n",
    "\n",
    "# Grab the data from the OpenML website\n",
    "# X will be our images and y will be the labels\n",
    "X, y = extract_training_samples('letters')\n",
    "\n",
    "# Make sure that every pixel in all of the images is a value between 0 and 1\n",
    "X = X / 255.\n",
    "\n",
    "# Use the first 60000 instances as training and the next 10000 as testing\n",
    "X_train, X_test = X[:60000], X[60000:70000]\n",
    "y_train, y_test = y[:60000], y[60000:70000]\n",
    "\n",
    "# There is one other thing we need to do, we need to\n",
    "# record the number of samples in each dataset and the number of pixels in each image\n",
    "X_train = X_train.reshape(60000,784)\n",
    "X_test = X_test.reshape(10000,784)\n",
    "\n",
    "print(\"Extracted our samples and divided our training and testing data sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7xwZ3sGQ2R5"
   },
   "source": [
    "To make sure the data downloaded, let's take a look at some of the pictures and labels in the set.  You can change the img_index value to different numbers and run the code as many times as you want to look at the label and picture for different training instances. Some letters are pretty clear (like, 8888 is definitely a \"Y\"), but others not as much (like, is 1234 a loopy \"L\" or a \"P\" or maybe even an \"E\" at a weird angle?) It's easy to see why this is such a hard task!\n",
    "\n",
    "But the important thing is that we know that the data downloaded correctly, so we're ready for the next steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "DWcb4sxAP2lT",
    "outputId": "bd5320a0-65fd-4f14-b235-40c9130bd2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label: m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f757c2fcda0>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1.3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_index = 14000 # <<<<<  You can update this value to look at other images\n",
    "img = X_train[img_index]\n",
    "print(\"Image Label: \" + str(chr(y_train[img_index]+96)))\n",
    "plt.imshow(img.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfT7zKZkUyYn"
   },
   "source": [
    "**STEP 2**\n",
    "\n",
    "Now that we've looked at the data, let's build a neural network that will be able to take the picture as input and output the corresponding letter.  In this case, we're going to stick to a multi-layer perceptron neural network, or MLP. \n",
    "\n",
    "Rather than trying to write all of it from scratch, we're going to import and use an existing library that provides some MLP tools for us. For a lot of AI problems, we can use existing libraries and tweak them to do something new rather than trying to write everything from scratch.\n",
    "\n",
    "Just to try this out, we'll create a neural network -- a MLP classifier -- with one hidden layer that has 50 neurons in it. We'll have it run through the training data 20 times (that's ```max_iter=20```) so it doesn't take too long. \n",
    "\n",
    "We also set a variety of other learning parameters to create the MLP.  You may recognize some of these parameters, like learning rate.  By changing any of these parameters, we can influence how the MLP performs in training and testing, but it's a good idea to try a default approach and then improve on it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rx-OxVm6Mjj2",
    "outputId": "16f17f35-7869-40d6-bba7-8fa866ff632c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created our first MLP network\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.1\n",
    "\n",
    "# These two lines import the ML libraries we need\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# This creates our first MLP with 1 hidden layer with 50 neurons and sets it to run through the data 20 times\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=20, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "print(\"Created our first MLP network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIBxv5cfWD9M"
   },
   "source": [
    "**STEP 3**\n",
    "\n",
    "Now that we've created the MLP, let's actually train it and test its ability to recognize some handwritten letters!  Go ahead and \"fit\" command to see how it will perform -- it might take a minute. As it's learning, it will print status update information about the learning process. You can see the loss value decreasing over each iteration, which means it's getting better at recognizing the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "DcyaMwrmWCk8",
    "outputId": "f63306d9-4b51-46e4-88de-ff1041c11f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06351395\n",
      "Iteration 2, loss = 0.64844650\n",
      "Iteration 3, loss = 0.56103245\n",
      "Iteration 4, loss = 0.51987725\n",
      "Iteration 5, loss = 0.49182099\n",
      "Iteration 6, loss = 0.47301057\n",
      "Iteration 7, loss = 0.45839220\n",
      "Iteration 8, loss = 0.44603836\n",
      "Iteration 9, loss = 0.43479721\n",
      "Iteration 10, loss = 0.42809575\n",
      "Iteration 11, loss = 0.41639233\n",
      "Iteration 12, loss = 0.40782908\n",
      "Iteration 13, loss = 0.40548360\n",
      "Iteration 14, loss = 0.39965983\n",
      "Iteration 15, loss = 0.39296832\n",
      "Iteration 16, loss = 0.38883219\n",
      "Iteration 17, loss = 0.38393955\n",
      "Iteration 18, loss = 0.37948343\n",
      "Iteration 19, loss = 0.37307616\n",
      "Iteration 20, loss = 0.37166732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.886500\n",
      "Test set score: 0.840800\n"
     ]
    }
   ],
   "source": [
    "# STEP 3.1\n",
    "\n",
    "mlp1.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp1.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSKEzW7YWT-l"
   },
   "source": [
    "You can try running this a couple of times and you'll see that the accuracy and loss will be in the same ballpark but slightly different each time. These differences are due to the fact that the neuron weights are randomly initialized in each run. But if you didn't change anything in the MLP structure above, you're probably seeing something like 89% accuracy on the training dataset and 84% accuracy on the testing dataset.\n",
    "\n",
    "That's actually pretty good -- it means that the network is guessing correctly 84% of time on letters it's never seen before! It learned! But let's see if we can make it even better. We can explore the dataset to figure out where things went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "0Ux7zg-iYAhv",
    "outputId": "a2c6c92b-04fd-4fe4-d972-1c42cf3e0d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f756efa87b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGC5dXyEAGCIEAoCEQACQEAgAEgIBQPL/DFKbKNsWbS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3.2\n",
    "\n",
    "# First let's initialize a list with all the predicted values from the training set\n",
    "y_pred = mlp1.predict(X_test)\n",
    "\n",
    "# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWe9nrfdaPBq"
   },
   "source": [
    "The color of each cell in the confusion matrix represents the number of elements in that cell -- a brighter color means more elements. The rows are the correct value and the columns are the predicted value. The numbers on the axes represent the 26 letters in the alphabet (though EMNIST just represents letters as numbers, so 0=A, 1=B, etc.), so the brightness of cell (0,0) represents the number of times that our network correctly predicted that an \"A\" is an \"A.\"  The diagonal line of cells shows when the predicted value is correct. It's good to see a bright line! \n",
    "\n",
    "If any of the cells off the diagonal are particularly bright, we can investigate more.\n",
    "\n",
    "For example, \"I\" and \"L\" may be easy to confuse (\"U\" and \"V\" are also good ones to try!), so let's look at some of the cases where that happened. You can change the code and run it as many times as you want to look at some examples of these mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "kGNJFFZmfS4I",
    "outputId": "348cf71e-a691-4c8e-b647-7bbc24990859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 37 times that the letter v was predicted to be the letter u.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiL8QFKEH0iK8ANJEX4gqf8H3BEHxW4q5LUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3.3\n",
    "\n",
    "# You can change this to any letters that you think the neural network may have confused...\n",
    "predicted_letter = 'u'\n",
    "actual_letter = 'v'\n",
    "\n",
    "\n",
    "# This code counts all mistakes for the letters above\n",
    "mistake_list = []\n",
    "for i in range(len(y_test)):\n",
    "  if (y_test[i] == (ord(actual_letter) - 96) and y_pred[i] == (ord(predicted_letter) - 96)):\n",
    "    mistake_list.append(i)\n",
    "print(\"There were \" + str(len(mistake_list)) + \" times that the letter \" + actual_letter + \" was predicted to be the letter \" + predicted_letter + \".\")\n",
    "\n",
    "# Once we know how many mistakes were made, we can change this to see an image of a particular one\n",
    "mistake_to_show = 30 # <<< e.g., change this to 3 if you want to see the 4th mistake\n",
    "\n",
    "# This code checks that the number mistake you asked for can be shown and if so, displays an image of it\n",
    "if (len(mistake_list)> mistake_to_show):\n",
    "  img = X_test[mistake_list[mistake_to_show]]\n",
    "  plt.imshow(img.reshape((28,28)))\n",
    "else:\n",
    "  print(\"Couldn't show mistake number \" + str(mistake_to_show + 1) + \" because there were only \" + str(len(mistake_list)) + \" mistakes to show!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAOTwscofh0X"
   },
   "source": [
    "So we can sort of see why the MLP neural network made the mistake here. Even as a human, I probably wouldn't know what some of these letters are unless I could see them in a word and guess from context. Neural networks have a hard job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QRU60vfX4XJ"
   },
   "source": [
    "On EMNIST, the state-of-the-art machine learning techniques get greater than 96% accuracy, so I think we can also get better than 84%. Let's make a new network called mlp2 and try some things out.\n",
    "\n",
    "One thing we can try is having more hidden layers and more neurons in the hidden layers. For example, to add another hidden layer of 50 neurons, you can do the following: ```hidden_layer_sizes=(50,50,)```. \n",
    "\n",
    "The other thing we can try is training for more epochs (or iterations). For example, we can change our ```max_iter=30```. \n",
    "\n",
    "You can play around with the structure to see what happens, but with more layers and more epochs, it takes a longer time for the network to train! So let's just start with 5 layers of 100 neurons each, trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "WiAUTXZrW2as",
    "outputId": "c30774a7-43e0-4a11-a7e6-147cdf374fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15599672\n",
      "Iteration 2, loss = 0.54834774\n",
      "Iteration 3, loss = 0.44784035\n",
      "Iteration 4, loss = 0.39865207\n",
      "Iteration 5, loss = 0.36308811\n",
      "Iteration 6, loss = 0.33567397\n",
      "Iteration 7, loss = 0.31904877\n",
      "Iteration 8, loss = 0.29485588\n",
      "Iteration 9, loss = 0.27846574\n",
      "Iteration 10, loss = 0.26938228\n",
      "Iteration 11, loss = 0.25802023\n",
      "Iteration 12, loss = 0.25935391\n",
      "Iteration 13, loss = 0.25040129\n",
      "Iteration 14, loss = 0.23778275\n",
      "Iteration 15, loss = 0.22900046\n",
      "Iteration 16, loss = 0.22848821\n",
      "Iteration 17, loss = 0.21585034\n",
      "Iteration 18, loss = 0.21573023\n",
      "Iteration 19, loss = 0.21042306\n",
      "Iteration 20, loss = 0.20923178\n",
      "Iteration 21, loss = 0.20420055\n",
      "Iteration 22, loss = 0.20746136\n",
      "Iteration 23, loss = 0.20814230\n",
      "Iteration 24, loss = 0.19709426\n",
      "Iteration 25, loss = 0.18931068\n",
      "Iteration 26, loss = 0.19564641\n",
      "Iteration 27, loss = 0.19622160\n",
      "Iteration 28, loss = 0.20256337\n",
      "Iteration 29, loss = 0.18332725\n",
      "Iteration 30, loss = 0.19266968\n",
      "Iteration 31, loss = 0.18690757\n",
      "Iteration 32, loss = 0.18365975\n",
      "Iteration 33, loss = 0.18847366\n",
      "Iteration 34, loss = 0.18030152\n",
      "Iteration 35, loss = 0.18514135\n",
      "Iteration 36, loss = 0.18055779\n",
      "Iteration 37, loss = 0.17975747\n",
      "Iteration 38, loss = 0.18615996\n",
      "Iteration 39, loss = 0.18265912\n",
      "Iteration 40, loss = 0.16979666\n",
      "Iteration 41, loss = 0.18479706\n",
      "Iteration 42, loss = 0.17704018\n",
      "Iteration 43, loss = 0.17667839\n",
      "Iteration 44, loss = 0.18577955\n",
      "Iteration 45, loss = 0.18380964\n",
      "Iteration 46, loss = 0.18452252\n",
      "Iteration 47, loss = 0.18314915\n",
      "Iteration 48, loss = 0.18020371\n",
      "Iteration 49, loss = 0.17523959\n",
      "Iteration 50, loss = 0.19086538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.942917\n",
      "Test set score: 0.883300\n"
     ]
    }
   ],
   "source": [
    "# STEP 3.4\n",
    "\n",
    "# Change some of the values in the below statement and re-run to see how they \n",
    "# affect performance!\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_VRX5lEWcwe"
   },
   "source": [
    "So that's a fair amount of improvement! By having more layers and more training epochs, we got an extra 5% or so of accuracy (remember, this might vary slightly because there is some randomness in the process). That might not seem like a lot, but eliminating 1 in 20 mistakes is actually a pretty big deal!\n",
    "\n",
    "You can keep playing with the neural network structure in this block and see if you can get even better accuracy. But this should also be good enough for now, so we're ready to apply it to our scanned letters.\n",
    "\n",
    "**STEP 4**\n",
    "\n",
    "First, we have to get the scanned letter dataset that we uploaded and saved on GitHub. You can see the raw scanned data set here: https://github.com/crash-course-ai/lab1-neural-networks/tree/master/letters. \n",
    "\n",
    "But, as you can see, those images are HUGE. So we've also done a bit of preprocessing to avoid having to download and process quite so much data. We've changed the size of every image to 128x128 pixels.The other thing you may notice is that the EMNIST dataset uses a dark background with light strokes, but our original scans have a white background with dark strokes. So, we also went ahead and inverted those colors. \n",
    "\n",
    "This modified dataset is what we'll use in our code and it is also available in the GitHub here: https://github.com/crash-course-ai/lab1-neural-networks/tree/master/letters_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "Bs1-wuqGHYRm",
    "outputId": "3665ba3d-db41-4960-9c79-fe22cc913bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lab1-neural-networks'...\n",
      "remote: Enumerating objects: 152, done.\u001b[K\n",
      "Receiving objects:   0% (1/152)   \r",
      "Receiving objects:   1% (2/152)   \r",
      "Receiving objects:   2% (4/152)   \r",
      "Receiving objects:   3% (5/152)   \r",
      "Receiving objects:   4% (7/152)   \r",
      "Receiving objects:   5% (8/152)   \r",
      "Receiving objects:   6% (10/152)   \r",
      "Receiving objects:   7% (11/152)   \r",
      "Receiving objects:   8% (13/152)   \r",
      "Receiving objects:   9% (14/152)   \r",
      "Receiving objects:  10% (16/152)   \r",
      "Receiving objects:  11% (17/152)   \r",
      "Receiving objects:  12% (19/152)   \r",
      "Receiving objects:  13% (20/152)   \r",
      "Receiving objects:  14% (22/152)   \r",
      "Receiving objects:  15% (23/152)   \r",
      "Receiving objects:  16% (25/152)   \r",
      "Receiving objects:  17% (26/152)   \r",
      "Receiving objects:  18% (28/152)   \r",
      "Receiving objects:  19% (29/152)   \r",
      "Receiving objects:  20% (31/152)   \r",
      "Receiving objects:  21% (32/152)   \r",
      "Receiving objects:  22% (34/152)   \r",
      "Receiving objects:  23% (35/152)   \r",
      "Receiving objects:  24% (37/152)   \r",
      "Receiving objects:  25% (38/152)   \r",
      "Receiving objects:  26% (40/152)   \r",
      "Receiving objects:  27% (42/152)   \r",
      "Receiving objects:  28% (43/152)   \r",
      "Receiving objects:  29% (45/152)   \r",
      "Receiving objects:  30% (46/152)   \r",
      "Receiving objects:  31% (48/152)   \r",
      "Receiving objects:  32% (49/152)   \r",
      "Receiving objects:  33% (51/152)   \r",
      "Receiving objects:  34% (52/152)   \r",
      "Receiving objects:  35% (54/152)   \r",
      "Receiving objects:  36% (55/152)   \r",
      "Receiving objects:  37% (57/152)   \r",
      "Receiving objects:  38% (58/152)   \r",
      "Receiving objects:  39% (60/152)   \r",
      "Receiving objects:  40% (61/152)   \r",
      "Receiving objects:  41% (63/152)   \r",
      "Receiving objects:  42% (64/152)   \r",
      "Receiving objects:  43% (66/152)   \r",
      "Receiving objects:  44% (67/152)   \r",
      "Receiving objects:  45% (69/152)   \r",
      "Receiving objects:  46% (70/152)   \r",
      "Receiving objects:  47% (72/152)   \r",
      "Receiving objects:  48% (73/152)   \r",
      "Receiving objects:  49% (75/152)   \r",
      "Receiving objects:  50% (76/152)   \r",
      "Receiving objects:  51% (78/152)   \r",
      "Receiving objects:  52% (80/152)   \r",
      "Receiving objects:  53% (81/152)   \r",
      "Receiving objects:  54% (83/152)   \r",
      "Receiving objects:  55% (84/152)   \r",
      "Receiving objects:  56% (86/152)   \r",
      "Receiving objects:  57% (87/152)   \r",
      "Receiving objects:  58% (89/152)   \r",
      "Receiving objects:  59% (90/152)   \r",
      "Receiving objects:  60% (92/152)   \r",
      "Receiving objects:  61% (93/152)   \r",
      "Receiving objects:  62% (95/152)   \r",
      "Receiving objects:  63% (96/152)   \r",
      "Receiving objects:  64% (98/152)   \r",
      "Receiving objects:  65% (99/152)   \r",
      "Receiving objects:  66% (101/152)   \r",
      "Receiving objects:  67% (102/152)   \r",
      "Receiving objects:  68% (104/152)   \r",
      "Receiving objects:  69% (105/152)   \r",
      "Receiving objects:  70% (107/152)   \r",
      "Receiving objects:  71% (108/152)   \r",
      "remote: Total 152 (delta 0), reused 0 (delta 0), pack-reused 152\u001b[K\n",
      "Receiving objects:  72% (110/152)   \r",
      "Receiving objects:  73% (111/152)   \r",
      "Receiving objects:  74% (113/152)   \r",
      "Receiving objects:  75% (114/152)   \r",
      "Receiving objects:  76% (116/152)   \r",
      "Receiving objects:  77% (118/152)   \r",
      "Receiving objects:  78% (119/152)   \r",
      "Receiving objects:  79% (121/152)   \r",
      "Receiving objects:  80% (122/152)   \r",
      "Receiving objects:  81% (124/152)   \r",
      "Receiving objects:  82% (125/152)   \r",
      "Receiving objects:  83% (127/152)   \r",
      "Receiving objects:  84% (128/152)   \r",
      "Receiving objects:  85% (130/152)   \r",
      "Receiving objects:  86% (131/152)   \r",
      "Receiving objects:  87% (133/152)   \r",
      "Receiving objects:  88% (134/152)   \r",
      "Receiving objects:  89% (136/152)   \r",
      "Receiving objects:  90% (137/152)   \r",
      "Receiving objects:  91% (139/152)   \r",
      "Receiving objects:  92% (140/152)   \r",
      "Receiving objects:  93% (142/152)   \r",
      "Receiving objects:  94% (143/152)   \r",
      "Receiving objects:  95% (145/152)   \r",
      "Receiving objects:  96% (146/152)   \r",
      "Receiving objects:  97% (148/152)   \r",
      "Receiving objects:  98% (149/152)   \r",
      "Receiving objects:  99% (151/152)   \r",
      "Receiving objects: 100% (152/152)   \r",
      "Receiving objects: 100% (152/152), 4.60 MiB | 20.77 MiB/s, done.\n",
      "Resolving deltas:   0% (0/59)   \r",
      "Resolving deltas:  13% (8/59)   \r",
      "Resolving deltas:  20% (12/59)   \r",
      "Resolving deltas:  47% (28/59)   \r",
      "Resolving deltas:  52% (31/59)   \r",
      "Resolving deltas:  55% (33/59)   \r",
      "Resolving deltas: 100% (59/59)   \r",
      "Resolving deltas: 100% (59/59), done.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "00.jpg\t10.jpg\t20.jpg\t30.jpg\t40.jpg\t50.jpg\t60.jpg\t70.jpg\t80.jpg\t90.jpg\n",
      "01.jpg\t11.jpg\t21.jpg\t31.jpg\t41.jpg\t51.jpg\t61.jpg\t71.jpg\t81.jpg\t91.jpg\n",
      "02.jpg\t12.jpg\t22.jpg\t32.jpg\t42.jpg\t52.jpg\t62.jpg\t72.jpg\t82.jpg\t92.jpg\n",
      "03.jpg\t13.jpg\t23.jpg\t33.jpg\t43.jpg\t53.jpg\t63.jpg\t73.jpg\t83.jpg\t93.jpg\n",
      "04.jpg\t14.jpg\t24.jpg\t34.jpg\t44.jpg\t54.jpg\t64.jpg\t74.jpg\t84.jpg\t94.jpg\n",
      "05.jpg\t15.jpg\t25.jpg\t35.jpg\t45.jpg\t55.jpg\t65.jpg\t75.jpg\t85.jpg\t95.jpg\n",
      "06.jpg\t16.jpg\t26.jpg\t36.jpg\t46.jpg\t56.jpg\t66.jpg\t76.jpg\t86.jpg\t96.jpg\n",
      "07.jpg\t17.jpg\t27.jpg\t37.jpg\t47.jpg\t57.jpg\t67.jpg\t77.jpg\t87.jpg\t97.jpg\n",
      "08.jpg\t18.jpg\t28.jpg\t38.jpg\t48.jpg\t58.jpg\t68.jpg\t78.jpg\t88.jpg\t98.jpg\n",
      "09.jpg\t19.jpg\t29.jpg\t39.jpg\t49.jpg\t59.jpg\t69.jpg\t79.jpg\t89.jpg\t99.jpg\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# STEP 4.1\n",
    "\n",
    "# Pulls the scanned data set from GitHub\n",
    "!git clone https://github.com/crash-course-ai/lab1-neural-networks.git\n",
    "!git pull\n",
    "!ls lab1-neural-networks/letters_mod\n",
    "!cd /content/lab1-neural-networks/letters_mod\n",
    "!pwd\n",
    "\n",
    "# Puts all the data in the \"files\" variable\n",
    "import os\n",
    "path, dirs, files = next(os.walk(\"/content/lab1-neural-networks/letters_mod/\"))\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8jot1iIMAC83"
   },
   "source": [
    "Here we'll read each image and add it to a list to hold the handwritten story. We'll also print this image to make sure we read it correctly -- feel free to change this index to see different letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "K0hPrlEvW5ZY",
    "outputId": "77e663cc-4643-4948-f8a0-a420709a8e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported the scanned images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7566d03278>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHX5JREFUeJzt3X1sZGeV5/HvqVvlcttud9vtpNPp\nTkgTwgyBhUCyIYgVYsmgBJYhrBZFQWgnOxuptRI7MCO0M8miFVrt/DFoR8MwK5bZFgHCKiKwmTCJ\nGAYGMiB2hUhIIMqEhIRA3hz6zYm7222n7apbZ/94nnt9b9lNJy5XuVz+faRWl29dVz2+7T51npf7\nHHN3REQylY1ugIj0FwUFESlRUBCREgUFESlRUBCREgUFESlRUBCRkq4FBTO71sweN7Mnzezmbr2P\niKwv68biJTNLgCeAdwPTwI+BD7n7o+v+ZiKyrqpdet0rgSfd/VcAZnYHcB2walAYsroPM9qlpogI\nwByzM+5+ztnO61ZQ2As8V/h6Gnhr8QQzOwAcABhmhLfa1V1qiogAfNfvfOblnLdhA43uftDdr3D3\nK2rUN6oZItKmW0HheeCCwtf74jER6XPdCgo/Bi4xs/1mNgTcANzTpfcSkXXUlTEFd2+a2X8Evg0k\nwBfc/WfdeC8RWV/dGmjE3b8JfLNbry8i3aEVjSJSoqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJS\noqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJSoqAg\nIiUKCj1QGR2lMrpcFq8yMgKVJPwpHpPVmYEZlZGRldcpe654fduu92oq27e/ovO3EgUFESnp2hbv\nAsnEBADp7GzpeGthYcW5rYWF/NOrNTfX/cZtJrEyenbdrF4nmQzXtnnocHhufj4/PXtstaHw7Y0l\nqhfsC+c/Nx3OKVzj/HsrCbTSbv0Um4YyBREpUabQRe0ZwmqZQ96XdVeGcAbZNco+0X1xkfTYTOkc\nqw1hr39NOO+hR8N5jaX8+fTw0fJrbt+eX+/knFCdPT12rAut33zWHBTM7ALgy8BuwIGD7v4ZM5sE\nvgpcBDwNXO/us2d6nUGW/bJ5THuzYJBMTOSPfakBgCWFpM2sh63sf1kwKHYBvNkMx/acF44dOozH\nYJCxavj1tqGhUtcjvGhr+cT4WhJ00n1oAh9390uBq4CPmNmlwM3Ave5+CXBv/FpENok1Zwrufgg4\nFB/PmdljwF7gOuCd8bTbgO8Df9JRKzep9nQ0G0i8/eG/YyIJU2unWqfDuTg7KtsAaLgGu4pm0pcA\nODdes8SWP8seWlwE4LJ6nceWQjbw8X9+HbB8/Svbt0PMFLIpzXR2luTS14bHjz7R7R9hU1mXMQUz\nuwh4M3AfsDsGDIDDhO7Fat9zADgAMIzm6EX6RcdBwczGgL8B/tDdT1qhP+zubma+2ve5+0HgIMC4\nTa56zmaXjI/HB2GRUjaOkGUJAAsxK6ixfN1qtryoSWBPdaz09aHmKaaSkFVdlo0RAK8bCtf1yz/5\nWwCmkjBAOZPOU4vZRY1wbRMz3r83fmP8nU0mJ0hfeLE7P8Qm0lFQMLMaISDc7u53xcNHzGyPux8y\nsz3A0TO/wmBLT54E+I2r5c5Nlp+bScOA2kTsRkiw6GEgcKQS1h1MJnUqrByMXWiF2YappHy9G+55\nECn69q8fAuCafZcDKCBEax5otJAS3Ao85u5/UXjqHuDG+PhG4O61N09Eeq2TTOHtwL8F/snMHorH\n/jPwZ8DXzOwm4Bng+s6auHlVhoeBwoq5mKYWBxKzrsKJ1ksrPuEkGLGQIaQephHrVsuv4alWGITc\nUdmWZxKZRQ/TvZPJchdjNg0DjhPJSJ5Z5KsYzfLVk1tZJ7MP/w9WyeGCq9f6uiKysbTMuYs8beFp\nC6tWw0Iad3DnfXsvp2ZJaUBxzOo0PF236cgTrZc4ET9FMwutJRZaS6Teyj91f5Oj6TxH0+V7ClJv\nMZsu5J+2ANPNU/njrP2L3sg/pTOrHXulEqvk05HZ9dtR2ZZP5barWy3/k6lZhZpVSL3FSGWonF0o\nSwAUFESkje596KLi2nuA6t7zAUhnXsg/xR9vhH+CK+vZZBlr/kStWy3PANo/PU+1TjNWGS4dS73F\ns83wqX9OEtpRKXxOZDMjWXvqVitNpwIM28qp1F82ws+25I18mjD7tH45Gcp6Kc5aHIoZTXF689l4\nLJs6bi0s5MuntzIFhS5qvxW6+fyvgXDvw0wauglX1pf/82ap+L62efmXa9EbHEnDCr8L216j4a38\nP/eL8Zw91TH2VcP7F7sy2X+WC2NqvdAK35ea5+l2Nn06lYzyRCM8vji+1sW15ff+ZSO81o6K5ef3\nymIrBgWGVqx1WPRGfo08/lsoIATqPohIiTKFLsoyhOxOvtaJsJgpnZ3lDy773fxxJruDz+N6/lcq\nmZjI37P9U6/6qgtoPvNcqT2fu+9ODrz6nQD8lyfuB+DtwxWmYjaQfcoXP/mzabzthQG6yfjRcspD\nu3dYyBieapwqfS/09r6OrKtTfM8nYpfu9UPbuOb8y+LR+fZv3dKUKYhIiXkfTMOM26S/1QZ4aUM2\nGLfKtW7fQKR0/it9m2otH9xcbUOXZOeO8F4vhTszfXExH2TLlmQXZcuAfxBO5x3D5OMHr62Fdi+0\nllYsGrr2wivC66/WR+/hXhFfe+6HQFj8tNr9JFmmkA0At46fKP87DJjv+p0PuvsVZztP3Ycuyjb5\nyAay8gCwsIBVw2h8cT/B7Ly17hPojaX8P77Vh/LXBUimJvPdirL/rMmuyXy9f/W8cDNr8/CRfJfp\nny2FWYSLqll7xvJgUFwZmMm6G/nrT+2iNRfXMbQ8b2OvXL/vbUBYWdo6fXrF89VXXQCQd6skUPdB\nREqUKXRRe/qcpe24r/zErBgWVyp4BzsK512D4ydKx5uHDudTpH4qfHq3Tp7KNx1pHj6Sn2u18Gvx\n+qHyWoeZdJ65+Im/Pw4gHmqe4sVWaPd/uiJsbpLsDD93OvNC/vqtxZU7WPeKjY1CzBSy+1Go1fIM\nIdk1CYSt8bRPpjIFEWmjTKEH8k/LuCVYsS9ffC7r/xcrR70i3sqnM7PxjGRqFxAygexTsPie7RmL\nVav5a2RbxWUrIaeSUaZi07IVmXuqY0zGRVHpzAttP3iCL/VuDOFMiu3KxhaKn4Yes6vV6nFsRQoK\nPdD+y1bczKP43HoOwmVdl2K3YNX2rDIz8vXpsGZhZJVl0S3CeWNWp12x+Ep4oxT3+PpZoOuTYivF\ngUcFgzJ1H0SkREFBREoUFESkREFBREoUFESkREFBREoUFESkREFBREoUFESkREFBREo6DgpmlpjZ\nT83sG/Hr/WZ2n5k9aWZfNbOhs72GiPSP9cgUPgY8Vvj6U8Cn3f01wCxw0zq8h4j0SEdBwcz2Af8K\n+Hz82oB3AXfGU24DPtDJe4hIb3V6l+RfAn8MbI9f7wKOu3u2u8g0sLfD95A+lJVvm0nn81oO2d2R\n2W3bVq0u36pcj9vPne6PuyTlzDopRf8+4Ki7P7jG7z9gZg+Y2QMN1raluYisv05L0b/fzN4LDAPj\nwGeAnWZWjdnCPuD51b7Z3Q8CByHs5txBO2QDZPUfppJR7pgLO0cnu88FID1yFAAbGlp9t2rpa2vO\nFNz9Fnff5+4XATcA/+juHwa+B3wwnnYjcHfHrRSRnunGzkt/AtxhZn8K/BS4tQvvIRssq/XwbPMU\nN8QRpS/GDCFT3NFoxa5M0rfWJSi4+/eB78fHvwKuXI/Xlf6VdR8mK1UeXIz/0du2dssKzcDqxWak\nP2lFo4iUaONWWZOsDFvNhrg87uGaTE0BkB47Fv4+eTIvmvubSudJf1GmICIlyhRkTYoFW7MaklmG\nUNzOPash8ZsK2Up/UVCQNSlWn/7oW/9NPBpqTFgSgoKNjuSFblcUipG+pe6DiJQoU5A1yUrSw8oq\nVJWx8Fw6OwunwgBjVth1tZLw0l+UKYhIiTKFLS7r/2e1J73ZzFcrZgVm6xbucKxZwhNLYZXia2vh\nkz+7W7Kodapwn0N8XhnC5qGgsMVlwSC73fmrT/9fYBsAaSwmW5xpeN1QqFidequHrZReUvdBREqU\nKWx1caVhljHsqGzLuw07KttKp043TzEcz59oe04GhzIFESlRprDVxXsRslubv7VQ59qR8ilZ5rC9\nkuTZg8YUBpcyBREpUaawxbVvfnLtyGJ+L8PFtTEAxiph+jHbQ0EGm4KCrHC8tXr9nkVvspiGAcnx\nGChk8Kj7ICIlyhS2uKzbkN3aPJPOc3l9tHTOdDN0J3Yn2/KFTBpoHFzKFESkRJnCFpdtl5ZtfjLX\nclqEexfOjZWf9lXDgGPDU060XgJgzOq9bqr0iILCFudL5RmF/XHGAZZnG7IbpGB5lWP+XKGoeGV7\n2OvdTy9X/KqMhkUPrbm59Wy2dJG6DyJSokxB1iTLHn5wGpKdOwBIj58onZNM7cq3YauMxIyhUCBG\n+pMyBREp6SgomNlOM7vTzH5uZo+Z2dvMbNLMvmNmv4h/T6xXY6X/vGM4ZAjFLKF63m6q5+0OWUIl\ngUpCa2FBWcIm0Wmm8BngW+7+28CbgMeAm4F73f0S4N74tQyovGQcYSbD6nWah4/QPHyEyvbtVEZH\n8sFG2RzWHBTMbAfwDmIBWXdfcvfjwHXAbfG024APdNpIEemdTjKF/cAx4Itm9lMz+7yZjQK73f1Q\nPOcwsLvTRkr/urw+lGcIvriYF38BsJFttObmaM3NURkZyQcbpb91EhSqwFuAz7n7m4F52roK7u7A\nqsUDzeyAmT1gZg80WFztFBHZAJ1MSU4D0+5+X/z6TkJQOGJme9z9kJntAY6u9s3ufhA4CDBuk6o6\nuslkG68cSZt5dpDdhl3ZERYxpUeW/+nbF0lJ/1pzpuDuh4HnzOy34qGrgUeBe4Ab47Ebgbs7aqGI\n9FSni5f+ALjdzIaAXwG/Twg0XzOzm4BngOs7fA/pQ9nGK9f/s39JZTjcD2FD5bqRycQEVMJGr+kL\nL25AK2UtOgoK7v4QcMUqT13dyevK5pHOzubdhla8qSq7Dbs1N7dcV6JthyfpX1rRKCIluvdhq8vK\nvnmaH8puj26v+3CidZqpeDt1tsmKVavLn/6VuAFLzBjCCbGuhDKETUOZgoiUKFPY6lohQ0jOOSc/\nlGUIt544D4CbdhwGYCoZzacit9nqm7vK5qegIAD89YNfB2AmtbyLkAWDbEOVZ5pNFZjdAtR9EJES\nZQoCwIXV5W3YGnHQMdu5OdtQZdG18HQrUKYgIiXKFASAh5fCAOJrqpU8M7h/sQHAlfUaAJfV69rN\neQtQpiAiJcoUBIA3DoV7Gaabp0jigqMsQ5hJQx2Iico2aiQb00DpGQWFLc6q4Vdguaswlq9FqMYA\nkE1RzqYLTCSakhx06j6ISIkyhS0uu4ux2FXIMoPMU41QYHZ/bUwrGrcAZQoiUqJMYauLg4qLHsYU\nppLRFYuXsvqSqbfyzVWyqUmrVvNsI7uPIqsp2ZqbAy142nQUFGRNspumvNlcsYFKVkw2GR+n9dLp\n0nPS/9R9EJESZQqyJtmUZN51IBSUheU9GoubrZS6FNLXlCmISIkyBVmTJG7jVj1vN83DR4DlDCFb\nEGXbtuWZQWtexWU3CwUFWZNrzr8sPjpCsnMHwPKgYiwO43Nz+b6NaAXkpqHug4iUKFOQtYkZQHX3\nOTQPHS49lU1RUjEsCee1FtR92CyUKYhISUdBwcz+yMx+ZmaPmNlXzGzYzPab2X1m9qSZfTWWlJNB\n00qhlZayhMroKJXRUWy4jg2H0vSthQVaCwt5uXrpf2sOCma2F/gocIW7vwFIgBuATwGfdvfXALPA\nTevRUBHpjU67D1Vgm5lVgRHgEPAuQll6gNuAD3T4HtKHKiMjVEbC3gpZhuCLiyE7mJujNTdHZbRw\nt2Wahj/S99Y80Ojuz5vZnwPPAi8B/wA8CBx392yZ2zSwt+NWStckkxMATDfDNOLFtVp+c1R2Q9QT\njbDz0mtroxyNuzAVBw5b8/OrvnbxeHHlo/S3TroPE8B1wH7gfGAUuPYVfP8BM3vAzB5osLjWZojI\nOutkSvJ3gKfc/RiAmd0FvB3YaWbVmC3sA55f7Zvd/SBwEGDcJnV/7Qb5Hz+5B4CL4+3RC62l/Pbo\nTOqWPz63bQMWGTydjCk8C1xlZiNmZsDVwKPA94APxnNuBO7urIki0kudjCncZ2Z3Aj8BmsBPCZ/8\nfwfcYWZ/Go/duh4Nle7IMoRnm2HLtWKlqEyxfuTJuB2bDK6OVjS6+yeBT7Yd/hVwZSevK72T7aBU\nKxzLCso+FsYbubwelpokVqFuWgQ76LSiUURKFPa3uGxbtR3x42GhtZRPRV5eD39nU5SwXGxWBpcy\nBREpUVDY4p5ozOeLkwCOpCFTyLIFCKXpG56SamfmLUHdhy1uf7W8JqFY8GUhbvVej7ssjanrsCUo\nUxCREmUKW1zWTSiuU8hWNFbi1GRxcHE6nieDS5mCiJQoUxggyc4dpMdPlI61V28CSCbCnZHp7Gy+\neClbyfhs8xR7Y7n59unHmXSefauseJTBokxBREqUKQyQYpZQGY6zCrW4gLli+dbr6exsPJaQYMWX\nKN378PBSmIWYrIS9EPZVx/jRaW2UMugUFAZM3l2Iawq8UKYt2TUJQPrCi/nX2aDiLxthAHFPMpR3\nG944VJ6uPNU6zX971/Xxq2e78wPIhlP3QURKlCkMmqwSU3uWX0lonSxPJ6bHjuWPa7EXURxcbHj5\nRcYqwzSfVoYw6JQpiEiJMoUBYrWh0tQjQHVf2Dc3PXIsfy4ZHwfAl5bPLQ4wZhlCJQ5CHkrDJq37\nqmN53cj2qU8ZHAoKA8RqVbwZb3OOA43N6VW2yMxKuZ1euYvSQmuJxbgZ90Rcr5CtTTiazisYbAHq\nPohIiTKFAVKsxZDsPheA9MjR/Fg2XZmtU0imduVdhezvkcoQi2m5RkMaBy93Vbat+royWJQpiEiJ\nMoUBUtm+nVa2WKlV3hDF6vV8RWMytQuAzz54NzUL4wWzcQ+FEYYYjwuaZuMAYza2cKL1kjKELUBB\nYYC05ubAwoxBcQ0CgDeWuwStOFh4YXUk31AlK/LyVOMU++O271kwmIml4qZUCGZLUPdBREqUKWxx\nY21dhf21sXxg8SUP6xiyDOH+xQZWDb8yKhg7uJQpiEjJWYOCmX3BzI6a2SOFY5Nm9h0z+0X8eyIe\nNzP7KzN70sweNrO3dLPx0rmF1hILrSXGK8OMV4aZSedJrEJilXxX5+nmKaabp7iyXsObTWUJA+7l\nZApfYmWJ+ZuBe939EuDe+DXAe4BL4p8DwOfWp5ki0itnDQru/gPgxbbD1wG3xce3AR8oHP+yBz8i\nlKXfs16NlfXXIKVBmmcHU8ko9y82uH+xQd1q1K3GZGWIycpQvumKDLa1DjTudvdD8fFhYHd8vBd4\nrnDedDx2COlLWdm4h+Iahsvqda6sh92aiqscAd6osg9bQscDjR62+HnFpYPM7ICZPWBmDzRY7LQZ\nIrJO1popHDGzPe5+KHYPsmVuzwMXFM7bF4+t4O4HgYMA4zapemQbrG4hK1hoLeWZQVYTIlu8tKCy\ncVvCWjOFe4Ab4+MbgbsLx38vzkJcBZwodDNEZBM4a6ZgZl8B3glMmdk08Engz4CvmdlNwDNAtpvn\nN4H3Ak8CC8Dvd6HNso6yZc6vGxrJj9164jwAbtpxGFhevPTeN70bKC+flsFz1qDg7h86w1NXr3Ku\nAx/ptFHSO9mKxuL9DVkwyArFZIOR6bFjUInVqFva6n1QaUWjiJTo3ocBUxkLdzjmt1DHT/bK6Eh+\n7K6nfwhAYstzjK1VXivLEBZiodlkahfpzAvdaLb0EWUKIlKiTGGAVEaWs4HK9u3hYCvkAK1Cpahs\nynEmnc/Lxu2orFyZtNBWij6deUG7OW8ByhREpESZwgDxRjPf7yDPGIaX60FavV46/0w7KbVnCEVp\nW5UpGTwKCgPEm4283kNlJKw7yHZ4TsbHSU+eBOCa8y8D4I7nfkglbt82ZiFgJFZZEQymmyEQJOec\nQzoz0+WfQjaaug8iUqJMYYBUtm3LM4NiDQiA9ORJqq++CIDmr54G4IYL304ldilWqxaVSSYmwmvM\nHtN2bFuAMgURKVGmMEBWqxDVevE4AJXRbbR+fbh0frJjfOXUYraMGfKy9sWKUml8PRlcCgoDxOp1\nKtvDisb2oi3p8eUK01kl6ub088sDkqfjnhaFexqymYusa9E6fkL3PGwB6j6ISIkyhUHS8hX3JlRG\nw1qE1vx83qXIytNXhofzLkcpY4jZQJYhFDMGrWgcfMoURKREmcIA8cbSimOt+fn8cfs4Q3Easn0K\n80znKUMYfMoURKREQUFEShQURKREQUFEShQURKREQUFEShQURKREQUFEShQURKTkrEHBzL5gZkfN\n7JHCsf9uZj83s4fN7OtmtrPw3C1m9qSZPW5m13Sr4SLSHS8nU/gScG3bse8Ab3D3NwJPALcAmNml\nwA3A6+P3/E8zSxCRTeOsQcHdfwC82HbsH9w924/rR4SS8wDXAXe4+6K7P0UoNHvlOrZXRLpsPcYU\n/j3w9/HxXuC5wnPT8ZiIbBId3SVpZp8AmsDta/jeA8ABgGFGznK2iPTKmoOCmf074H3A1bEEPcDz\nwAWF0/bFYyu4+0HgIMC4Tfpq54hI762p+2Bm1wJ/DLzf3Ys34t8D3GBmdTPbD1wC3N95M0WkV86a\nKZjZV4B3AlNmNg18kjDbUAe+Y6HC0I/c/T+4+8/M7GvAo4RuxUfcXTt9imwitpz5b5xxm/S32tUb\n3QyRgfZdv/NBd7/ibOdpRaOIlCgoiEiJgoKIlCgoiEiJgoKIlCgoiEiJgoKIlCgoiEhJXyxeMrNj\nwDwws9FtAaZQO4rUjrLN3I5Xufs5ZzupL4ICgJk98HJWW6kdaofa0d12qPsgIiUKCiJS0k9B4eBG\nNyBSO8rUjrKBb0ffjCmISH/op0xBRPpAXwQFM7s21ol40sxu7tF7XmBm3zOzR83sZ2b2sXh80sy+\nY2a/iH9P9Kg9iZn91My+Eb/eb2b3xWvyVTMb6kEbdprZnbGmx2Nm9raNuB5m9kfx3+QRM/uKmQ33\n6nqcoc7JqtfAgr+KbXrYzN7S5Xb0pN7KhgeFWBfis8B7gEuBD8X6Ed3WBD7u7pcCVwEfie97M3Cv\nu18C3Bu/7oWPAY8Vvv4U8Gl3fw0wC9zUgzZ8BviWu/828KbYnp5eDzPbC3wUuMLd3wAkhFoivboe\nX2JlnZMzXYP3ELYcvISwCfHnutyO3tRbcfcN/QO8Dfh24etbgFs2oB13A+8GHgf2xGN7gMd78N77\nCL9s7wK+ARhhYUp1tWvUpTbsAJ4ijjMVjvf0erBcJmCSsF3gN4Brenk9gIuAR852DYD/BXxotfO6\n0Y625/41cHt8XPo/A3wbeNta33fDMwX6oFaEmV0EvBm4D9jt7ofiU4eB3T1owl8SNsJtxa93Acd9\nueBOL67JfuAY8MXYjfm8mY3S4+vh7s8Dfw48CxwCTgAP0vvrUXSma7CRv7tdq7fSD0FhQ5nZGPA3\nwB+6+8nicx7CblenZ8zsfcBRd3+wm+/zMlSBtwCfc/c3E5adl7oKPboeE4RKY/uB84FRVqbRG6YX\n1+BsOqm38nL0Q1B42bUi1puZ1QgB4XZ3vysePmJme+Lze4CjXW7G24H3m9nTwB2ELsRngJ1mlu22\n3YtrMg1Mu/t98es7CUGi19fjd4Cn3P2YuzeAuwjXqNfXo+hM16Dnv7uFeisfjgFq3dvRD0Hhx8Al\ncXR5iDBgck+339TC3vS3Ao+5+18UnroHuDE+vpEw1tA17n6Lu+9z94sIP/s/uvuHge8BH+xhOw4D\nz5nZb8VDVxO26u/p9SB0G64ys5H4b5S1o6fXo82ZrsE9wO/FWYirgBOFbsa661m9lW4OGr2CAZX3\nEkZTfwl8okfv+S8IaeDDwEPxz3sJ/fl7gV8A3wUme3gd3gl8Iz5+dfyHfRL4P0C9B+9/GfBAvCZ/\nC0xsxPUA/ivwc+AR4H8Taoz05HoAXyGMZTQI2dNNZ7oGhAHhz8bf238izJh0sx1PEsYOst/Xvy6c\n/4nYjseB93Ty3lrRKCIl/dB9EJE+oqAgIiUKCiJSoqAgIiUKCiJSoqAgIiUKCiJSoqAgIiX/H03x\nG6+SYXI4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 4.2\n",
    "\n",
    "# These libraries let us import the letters, resize them, and print them out\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This code processes all the scanned images and adds them to the handwritten_story\n",
    "handwritten_story = []\n",
    "for i in range(len(files)):\n",
    "  img = cv2.imread(\"/content/lab1-neural-networks/letters_mod/\"+files[i],cv2.IMREAD_GRAYSCALE)\n",
    "  handwritten_story.append(img)\n",
    "\n",
    "print(\"Imported the scanned images.\")\n",
    "\n",
    "plt.imshow(handwritten_story[94])  #<--- Change this index to see different letters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L85HdxpOoM0t"
   },
   "source": [
    "Well, to the naked eye this looks similar enough to the EMNIST letters and these even seem like they're MORE clear and bright. So, let's try actually getting the story!\n",
    "\n",
    "We're going to put the handwritten story into our trained MLP and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2TPLm8eYoq38",
    "outputId": "10725fe1-f3ef-4709-c46f-c9d5f56410cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to typed story complete!\n",
      "thejfnultjivjourjqouerjsupeliesjijrclljinjluvejthejwnyjyquzjbnttezyjdcesjslawlyjnndjttenjnlljntjancr\n"
     ]
    }
   ],
   "source": [
    "# STEP 4.3\n",
    "\n",
    "# These are libraries we need to do some math on the image\n",
    "# to be able to give it to the MLP in the right format and to resize it to 28x28 pixels\n",
    "import numpy\n",
    "import cv2\n",
    "\n",
    "typed_story = \"\"\n",
    "for letter in handwritten_story:\n",
    "    letter = cv2.resize(letter, (28,28), interpolation = cv2.INTER_CUBIC)\n",
    "    single_item_array = (numpy.array(letter)).reshape(1,784)\n",
    "    prediction = mlp2.predict(single_item_array)\n",
    "    typed_story = typed_story + str(chr(prediction[0]+96))\n",
    "    \n",
    "print(\"Conversion to typed story complete!\")\n",
    "print(typed_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOVBUJagqLus"
   },
   "source": [
    "And... that makes no sense, so what are we doing wrong?\n",
    "\n",
    "Well, first of all, John-Green-bot's handwritten story had some empty spaces between words. We never actually trained our model on empty spaces, just the 26 letters, so it wouldn't be able to detect these.\n",
    "\n",
    "But blank pages should be easy to detect. After all, unlike handwritten letters, all blank images should be exactly the same. So, we'll just check each image to see if it's a blank space. And if it is, we'll add a space to our story. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "U8BGJtQRq1xi",
    "outputId": "5047fa13-c70e-4cc6-8c85-57ead59ae6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to typed story complete!\n",
      "the fnult iv our qouer supelies i rcll in luve the wny yquz bnttezy dces slawly nnd tten nll nt ancr\n"
     ]
    }
   ],
   "source": [
    "# STEP 4.4\n",
    "\n",
    "# This is a library we need to do some math on the image to be able to give it to the MLP in the right format\n",
    "import numpy\n",
    "\n",
    "typed_story = \"\"\n",
    "for letter in handwritten_story:\n",
    "  letter = cv2.resize(letter, (28,28), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "  #this bit of code checks to see if the image is just a blank space by looking at the color of all the pixels summed\n",
    "  total_pixel_value = 0\n",
    "  for j in range(28):\n",
    "    for k in range(28):\n",
    "      total_pixel_value += letter[j,k]\n",
    "  if total_pixel_value < 20:\n",
    "    typed_story = typed_story + \" \"\n",
    "  else:         #if it NOT a blank, it actually runs the prediction algorithm on it\n",
    "    single_item_array = (numpy.array(letter)).reshape(1,784)\n",
    "    prediction = mlp2.predict(single_item_array)\n",
    "    typed_story = typed_story + str(chr(prediction[0]+96))\n",
    "    \n",
    "print(\"Conversion to typed story complete!\")\n",
    "print(typed_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZ3RdoEarHyM"
   },
   "source": [
    "This looks better. There are separate words, and the first word is \"The.\" But there's still something going wrong.\n",
    "\n",
    "The papers we scanned were really big compared to the handwritten samples that were used to train EMNIST. We resized them, but that doesn't seem to be enough. So we should try processing these images in the same way that EMNIST did.\n",
    "\n",
    "So, let's take a look at the orginal EMNIST paper... The paper (https://arxiv.org/abs/1702.05373v1) describes the steps we need to take to process these images in one of the figures. Essentially, we need to make the strokes more blurry, we need to put the letter at the very center of the image and crop out the rest, and we need to resize each one to be 28x28 pixels.\n",
    "\n",
    "We'll do this processing and then print out one letter to see how it turned out. As always, we can change the index in the code to see a different letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "RQ1SqowzKeHG",
    "outputId": "6fc1abd4-6cb1-446e-e394-1f1e7f717d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed the scanned images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7566c6dfd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADkBJREFUeJzt3V+MXOV9xvHn2fV6LWxITU1WLpiQ\nUicSQq2ptk4lUJSWJiIolUkuaJAauRKKuQhSIuWiiFYqF71AbULERRrJKU5MlZIiJQhUoQTXQUFR\nIsRCXTBxWwxyih3jPxCIY4K9f3692EO0mJ33DDNn5szy+36k1c6c95w5Pw5+9szMe877OiIEIJ+x\ntgsA0A7CDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqVXD3NlqT8YarR3mLlPwWOFvuD3YnY/X\nnD9KF5DGQs2Ll2uPubma7fN5Q6d1Ns509T+9r/Dbvk7S3ZLGJf1zRNxZWn+N1upDvrafXY6msfH+\ntq8LgcsBG1t7XudNVw3277svWFdeYb7zf1u88UbNi5f/u+dPnChvn9DjsbfrdXt+2297XNJXJX1c\n0hWSbrJ9Ra+vB2C4+vnMv1XSwYh4ISLOSvq2pG3NlAVg0PoJ/8WSXlzy/HC17C1s77A9Y3tmVmf6\n2B2AJg382/6I2BkR0xExPaHJQe8OQJf6Cf8RSZuWPL+kWgZgBegn/E9I2mz7/bZXS/q0pIeaKQvA\noPXcDxQRc7ZvlfR9LXb17YqIZxurbMSsuuzSjm3P3fK2rzreYmF1zWhJNc1123/gysMd2y5d90r5\ntaP8938hyl3GG9cc6nn7V+c6d1FK0oFXp4rt/sfpYvvqR5/u2BZzs8VtlWCEq746gSPiYUkPN1QL\ngCHi8l4gKcIPJEX4gaQIP5AU4QeSIvxAUkO9n38lm3vvezq2feMvvlrc9qrV5fvOF1R3X3vZpCc6\ntk24v9uNZ2O+v3Z1bl/j8j+/2Y3l1/79T3y+2P6BR4vN6XHmB5Ii/EBShB9IivADSRF+ICnCDyRF\nV18DZqN8GO879TvF9pNz5xfb7z90VbH9tVOF0XuLW9beTVy7/cLxNcX2tS92Pr+c3vLrmlcvu+zf\ny12BMV9oT3DLbh3O/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFP38Dfi/2QuL7d/42/IUhu+Z+Xmx\nfeq1Y8X2i04X+strp8GuMd7nDMQLhf70sfJVBK7Zd8yWb5WmJ7+MMz+QFOEHkiL8QFKEH0iK8ANJ\nEX4gKcIPJNVXP7/tQ5JOSZqXNBcR5TmT36Xq7uef/EW5P3ruxXI/v2v6w6PUl75Qvue9VumeeEly\nH+ePmpeu68ev1e81Du9yTVzk8ycRcbKB1wEwRLztB5LqN/wh6RHbT9re0URBAIaj37f910TEEdvv\nlbTH9n9HxGNLV6j+KOyQpDXqPNYcgOHq68wfEUeq38clPSBp6zLr7IyI6YiYntBkP7sD0KCew297\nre3z33ws6WOS9jdVGIDB6udt/5SkB2y/+Tr/GhHfa6QqAAPXc/gj4gVJf9BgLSPNha702ai55712\n8Pua8efb7K6uG9++ZopujC66+oCkCD+QFOEHkiL8QFKEH0iK8ANJMXR3l8Z//nLHtkdOXlHeuHYe\n7Jq+QKaTxgBw5geSIvxAUoQfSIrwA0kRfiApwg8kRfiBpOjn71K83nka7Lkoj1D06u+tLrZv+AH9\n+Bg+zvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBT9/N0qjJ993qqzxU1f31g3djcwfJz5gaQIP5AU\n4QeSIvxAUoQfSIrwA0kRfiCp2vDb3mX7uO39S5ZdaHuP7eeq3+sHW+ZoG1MUf2SVf4AWdHPm/6ak\n685ZdpukvRGxWdLe6jmAFaQ2/BHxmKRXzlm8TdLu6vFuSTc0XBeAAev1M/9URBytHr8kaaqhegAM\nSd9f+EVEqDAbne0dtmdsz8zqTL+7A9CQXsN/zPZGSap+H++0YkTsjIjpiJieUHmgSwDD02v4H5K0\nvXq8XdKDzZQDYFi66eq7T9JPJH3Q9mHbN0u6U9JHbT8n6c+q5wBWkNr7+SPipg5N1zZcy2hb6H1s\n/aAvHyOIK/yApAg/kBThB5Ii/EBShB9IivADSTF0dwPGXO4GjFXl9rG1a4vtHh8vF3Bx51srYnKi\nvO/XThfbF06ee0/XuSt0HtJckmJurnPbGS73bhNnfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iin7+\nBizU3LN76w0PF9t/+OHNxfZVY+W+9D/f8IOObZsmXi5u+5PT5X0/+dqlxfY6+7/3wY5t7/vSU8Vt\nF87Oll98Yb6XklDhzA8kRfiBpAg/kBThB5Ii/EBShB9IivADSdHP362xzn35dffz/+UFB4rtHzrv\nYLH9jSjfk3/o7IaObf909E+L29Zdo1D331Znbl1h+7HyuceFYy5JUb78ATU48wNJEX4gKcIPJEX4\ngaQIP5AU4QeSIvxAUrX9/LZ3SfqEpOMRcWW17A5Jn5V0olrt9ogo37S+0hWn6C73R1/9+I5i+yV3\nlcflH/t157HvJWns9cL490ePF7etE/P93TN/+fx/dmxbYNz+VnVz5v+mpOuWWf6ViNhS/by7gw+8\nC9WGPyIek1QzbQuAlaafz/y32n7a9i7b6xurCMBQ9Br+r0m6XNIWSUclfbnTirZ32J6xPTMrPuMB\no6Kn8EfEsYiYj4gFSV+XtLWw7s6ImI6I6QlN9longIb1FH7bG5c8/aSk/c2UA2BYuunqu0/SRyRt\nsH1Y0t9J+ojtLZJC0iFJtwywRgADUBv+iLhpmcX3DKCWFavunvjYf0Gx3T/+cXn7mv0zej16wRV+\nQFKEH0iK8ANJEX4gKcIPJEX4gaQYunsIanoCgVZw5geSIvxAUoQfSIrwA0kRfiApwg8kRfiBpOjn\n79Z4eXhtYKXhzA8kRfiBpAg/kBThB5Ii/EBShB9IivADSdHP3yVPru7YtmGSeUyx8nDmB5Ii/EBS\nhB9IivADSRF+ICnCDyRF+IGkavv5bW+SdK+kKS3OFr0zIu62faGkf5N0maRDkm6MiF8MrtR2zV9y\nUce2T63fU9x2T/xR0+UAfevmzD8n6YsRcYWkP5b0OdtXSLpN0t6I2Cxpb/UcwApRG/6IOBoRT1WP\nT0k6IOliSdsk7a5W2y3phkEVCaB57+gzv+3LJF0l6XFJUxFxtGp6SYsfCwCsEF2H3/Y6Sd+R9IWI\n+OXStogILX4fsNx2O2zP2J6Z1Zm+igXQnK7Cb3tCi8H/VkR8t1p8zPbGqn2jpOPLbRsROyNiOiKm\nJzTZRM0AGlAbftuWdI+kAxFx15KmhyRtrx5vl/Rg8+UBGJRubum9WtJnJD1je1+17HZJd0q63/bN\nkn4m6cbBlDgaYqzzPNtrPDvESoBm1IY/In4kqdO//GubLQfAsHCFH5AU4QeSIvxAUoQfSIrwA0kR\nfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICmm6G7AWY2XV1h2gLMl3HmsgMXt614AeOc4\n8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUvTzN+DE3AXF9gteqOmnpx8fLeDMDyRF+IGkCD+QFOEH\nkiL8QFKEH0iK8ANJ1fbz294k6V5JU1q8M31nRNxt+w5Jn5V0olr19oh4eFCFjrKDZ6aK7ecfPlN+\nAe7nRwu6uchnTtIXI+Ip2+dLetL2nqrtKxHxpcGVB2BQasMfEUclHa0en7J9QNLFgy4MwGC9o8/8\nti+TdJWkx6tFt9p+2vYu2+s7bLPD9oztmVnVvP0FMDRdh9/2OknfkfSFiPilpK9JulzSFi2+M/jy\ncttFxM6ImI6I6QlNNlAygCZ0FX7bE1oM/rci4ruSFBHHImI+IhYkfV3S1sGVCaBpteG3bUn3SDoQ\nEXctWb5xyWqflLS/+fIADEo33/ZfLekzkp6xva9adrukm2xv0WL33yFJtwykwhEx/vyRjm0P/v21\nxW1/a//zxfb5nioC+tPNt/0/krRcR3TKPn3g3YIr/ICkCD+QFOEHkiL8QFKEH0iK8ANJMXR3l+Zf\nfqVj2/n3P96xTeqiH59bdtECzvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kJRjiH3Mtk9I+tmSRRsk\nnRxaAe/MqNY2qnVJ1NarJmt7X0Rc1M2KQw3/23Zuz0TEdGsFFIxqbaNal0RtvWqrNt72A0kRfiCp\ntsO/s+X9l4xqbaNal0RtvWqltlY/8wNoT9tnfgAtaSX8tq+z/T+2D9q+rY0aOrF9yPYztvfZnmm5\nll22j9vev2TZhbb32H6u+r3sNGkt1XaH7SPVsdtn+/qWattk+1HbP7X9rO3PV8tbPXaFulo5bkN/\n2297XNL/SvqopMOSnpB0U0T8dKiFdGD7kKTpiGi9T9j2hyX9StK9EXFltewfJL0SEXdWfzjXR8Rf\nj0htd0j6VdszN1cTymxcOrO0pBsk/ZVaPHaFum5UC8etjTP/VkkHI+KFiDgr6duStrVQx8iLiMck\nnTuKyDZJu6vHu7X4j2foOtQ2EiLiaEQ8VT0+JenNmaVbPXaFulrRRvgvlvTikueHNVpTfoekR2w/\naXtH28UsY6qaNl2SXpI01WYxy6iduXmYzplZemSOXS8zXjeNL/ze7pqI+ENJH5f0uert7UiKxc9s\no9Rd09XMzcOyzMzSv9Hmset1xuumtRH+I5I2LXl+SbVsJETEker3cUkPaPRmHz725iSp1e/jLdfz\nG6M0c/NyM0trBI7dKM143Ub4n5C02fb7ba+W9GlJD7VQx9vYXlt9ESPbayV9TKM3+/BDkrZXj7dL\nerDFWt5iVGZu7jSztFo+diM343VEDP1H0vVa/Mb/eUl/00YNHer6XUn/Vf0823Ztku7T4tvAWS1+\nN3KzpN+WtFfSc5L+Q9KFI1Tbv0h6RtLTWgzaxpZqu0aLb+mflrSv+rm+7WNXqKuV48YVfkBSfOEH\nJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCp/wc7x3S2OjwHIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 4.5\n",
    "\n",
    "# These steps process the scanned images to be in the same format and have the same properties as the EMNIST images\n",
    "# They are described by the EMNIST authors in detail here: https://arxiv.org/abs/1702.05373v1\n",
    "processed_story = []\n",
    "\n",
    "for img in handwritten_story:\n",
    "  #step 1: Apply Gaussian blur filter\n",
    "  img = cv2.GaussianBlur(img, (7,7), 0)\n",
    "  \n",
    "  #steps 2 and 3: Extract the Region of Interest in the image and center in square\n",
    "  points = cv2.findNonZero(img)\n",
    "  x, y, w, h = cv2.boundingRect(points)\n",
    "  if (w > 0 and h > 0):\n",
    "    if w > h:\n",
    "      y = y - (w-h)//2\n",
    "      img = img[y:y+w, x:x+w]\n",
    "    else:\n",
    "      x = x - (h-w)//2\n",
    "      img = img[y:y+h, x:x+h]\n",
    "     \n",
    "  #step 4: Resize and resample to be 28 x 28 pixels\n",
    "  img = cv2.resize(img, (28,28), interpolation = cv2.INTER_CUBIC)\n",
    "  \n",
    "  #step 5: Normalize pixels and reshape before adding to the new story array\n",
    "  img = img/255\n",
    "  img = img.reshape((28,28))\n",
    "  processed_story.append(img)\n",
    "\n",
    "print(\"Processed the scanned images.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(processed_story[4]) #<<< change this index if you want to see a different letter from the story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhobxHEoKqXt"
   },
   "source": [
    "So even though the letter looks less clear now to our human eyes, it actually does look much more similar to the letters in the EMNIST dataset, which is good for our neural network. The edges of the letter are kind of fuzzy and it's centered in the square.\n",
    "\n",
    "Okay, so now we can try the processing the story with our neural network one more time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wFH4kBA9ar-O",
    "outputId": "8b350620-91f8-4f9d-8c89-11f3fef6367c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to typed story complete!\n",
      "the fault jn our pcwer supplies i fell in love the way your battery dies slowly and then all af once\n"
     ]
    }
   ],
   "source": [
    "# STEP 4.6\n",
    "\n",
    "# This is a library we need to do some math on the image to be able to give it to the MLP in the right format\n",
    "import numpy\n",
    "\n",
    "typed_story = \"\"\n",
    "for letter in processed_story:\n",
    "  #this bit of code checks to see if the image is just a blank space by looking at the color of all the pixels summed\n",
    "  total_pixel_value = 0\n",
    "  for j in range(28):\n",
    "    for k in range(28):\n",
    "      total_pixel_value += letter[j,k]\n",
    "  if total_pixel_value < 20:\n",
    "    typed_story = typed_story + \" \"\n",
    "  else:         #if it NOT a blank, it actually runs the prediction algorithm on it\n",
    "    single_item_array = (numpy.array(letter)).reshape(1,784)\n",
    "    prediction = mlp2.predict(single_item_array)\n",
    "    typed_story = typed_story + str(chr(prediction[0]+96))\n",
    "    \n",
    "print(\"Conversion to typed story complete!\")\n",
    "print(typed_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWADSaLh_uho"
   },
   "source": [
    "We can definitely pick out more of the words! There are still some mistakes because our neural network accuracy wasn't 100%, so we would expect to see about the same rate of error here (maybe even a bit more, since these letters were originally created at a very different size). But, looking at the context and knowing which letters are likely to be mistaken for one another, we can still read the story.\n",
    "\n",
    "Looks like it starts: \n",
    "\"The Fault in Our Power Supplies” \n",
    "“I fell in love the way your battery dies, slowly and then all at once”\n",
    "\n",
    "No matter how many times you run the predict code above, it's going to make the exact same mistakes in the final printed string. That's because once a network is trained, it's not really random anymore -- it's always applying the same math to the pixels to make its prediction. To get a different prediction, you have to rerun the code above where you trained mlp2.\n",
    "\n",
    "We hope that you mess around with some of this code on your own and try to get better accuracy than we got on the scanned story!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Networks Lab: Crash Course AI #5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
